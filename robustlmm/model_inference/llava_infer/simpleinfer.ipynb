{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/songmingyang/anaconda3/envs/smoe/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-04 19:49:40,411] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/songmingyang/anaconda3/envs/smoe/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "from llava.model import LlavaLlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import torch\n",
    "import argparse\n",
    "import datetime\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "# from peft import PeftModel\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from typing import Dict, Sequence, Optional,List\n",
    "# from accelerate import PartialState,Accelerator\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import threading\n",
    "\n",
    "# from mhr.alignment.models.llava_v1_5.llava.utils import disable_torch_init\n",
    "# from mhr.alignment.models.llava_v1_5.llava.model.builder import load_pretrained_model\n",
    "# from mhr.alignment.models.llava_v1_5.llava.conversation import conv_templates, SeparatorStyle\n",
    "# from mhr.alignment.models.llava_v1_5.llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "# from mhr.alignment.models.llava_v1_5.llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from transformers import HfArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/mnt/petrelfs/songmingyang/anaconda3/envs/smoe/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.88s/it]\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_path, device='cuda', peft_model_path=None):\n",
    "    model_name = get_model_name_from_path(model_path)\n",
    "    tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "                model_path=model_path, \n",
    "                model_base=None, \n",
    "                model_name=model_name,\n",
    "                load_8bit=False, \n",
    "                load_4bit=False, \n",
    "                device=device,\n",
    "            )\n",
    "    if peft_model_path:\n",
    "        model = PeftModel.from_pretrained(model, peft_model_path, adapter_name=\"dpo\")\n",
    "        print(\"peft model loaded\")\n",
    "    model.to(torch.float16)\n",
    "    return tokenizer, model, image_processor, context_len\n",
    "\n",
    "\n",
    "image_path = \"/mnt/petrelfs/songmingyang/code/mm/robustLMM/robustlmm/model_inference/llava_infer/samples/test1.jpg\"\n",
    "inp = \"Please Describe this image in detail\"\n",
    "# print(inp)\n",
    "model_path = \"/mnt/petrelfs/songmingyang/songmingyang/model/others/llava-v1.5-7b\"\n",
    "tokenizer, model, image_processor, context_len = initialize_model(model_path, device=\"cuda\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/mnt/petrelfs/songmingyang/code/mm/robustLMM/robustlmm/model_inference/llava_infer/samples/test2.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_tensor = process_images([image], image_processor, model.config).to(model.dtype).to(model.device)\n",
    "\n",
    "conv_mode = \"llava_v1\"\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "inp = inp.strip().replace('\\n', ' ').replace(DEFAULT_IMAGE_TOKEN, '').replace(DEFAULT_IM_START_TOKEN, '').replace(DEFAULT_IM_END_TOKEN, '').replace(\"<image>\",\"\")\n",
    "assert DEFAULT_IMAGE_TOKEN not in inp\n",
    "assert image is not None\n",
    "\n",
    "if image is not None and DEFAULT_IMAGE_TOKEN not in inp:\n",
    "    # first message\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n",
    "    else:\n",
    "        inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    image = None\n",
    "else:\n",
    "    # later messages\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "assert prompt.count(DEFAULT_IMAGE_TOKEN) == 1\n",
    "assert prompt.count(DEFAULT_IM_START_TOKEN) == 0\n",
    "# assert prompt.count(\"\\n\") == 0\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n",
    "\n",
    "\n",
    "generation_num=1\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]   \n",
    "    \n",
    "\n",
    "\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "\n",
    "with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "                inputs=input_ids,\n",
    "                images=image_tensor,\n",
    "                do_sample=False,\n",
    "                temperature=0,\n",
    "                max_new_tokens=512,\n",
    "                use_cache=True,\n",
    "                stopping_criteria=[stopping_criteria],\n",
    "                )\n",
    "        outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The image displays a graph showing the sales of a product over time. The graph is divided into two sections, one for the sales of the product and the other for the sales of the product's components. The sales of the product are shown in blue, while the sales of the components are shown in green.\\n\\nThe graph shows a steady increase in sales from 2005 to 2010, with a slight dip in 2009. The sales of the components have a similar pattern, with a slight dip in 2009 as well. The graph also includes a line that shows the sales of the product in 2011, which is slightly higher than the previous year.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 336, 336])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape\n",
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smoe",
   "language": "python",
   "name": "smoe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
