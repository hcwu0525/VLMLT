{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/songmingyang/anaconda3/envs/smoe/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-27 19:13:39 | INFO | qwen_vl_utils.vision_process | set VIDEO_TOTAL_PIXELS: 90316800\n",
      "2025-01-27 19:13:39 | ERROR | ipykernel.comm | No such comm target registered: jupyter.widget.control\n",
      "2025-01-27 19:13:39 | WARNING | Comm | No such comm: d410984e-eeed-419a-8ca5-b4c6c35a85fa\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "2025-01-27 19:13:42 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:08<00:33,  8.49s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:14<00:20,  6.99s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:20<00:13,  6.58s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:26<00:06,  6.41s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:28<00:00,  4.75s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:28<00:00,  5.70s/it]\n",
      "2025-01-27 19:14:10 | ERROR | stderr | \n",
      "2025-01-27 19:14:21 | INFO | stdout | [\"The image is an illustration of an anime-style character. The character has long, flowing blue hair and is wearing a beige coat with a high collar and a black dress underneath. The coat has a brownish hue and features a large, decorative button on the front. The character is holding a large, mechanical-looking sword with a black and silver design. The sword has a long, cylindrical handle and a sharp blade. In the background, there is a floating blue crystal or gemstone, and the character appears to be in a snowy or cold environment, as suggested by the white, snowy landscape and the character's attire. The overall style is vibrant and detailed, with a focus on the character's expression and the mechanical elements of the sword.\"]\n",
      "2025-01-27 19:15:53 | INFO | stdout | [\"The image is an illustration of an anime-style character. The character has long, flowing blue hair and is wearing a beige coat with a high collar and a black dress underneath. The coat has a brownish hue and features a large, decorative button on the front. The character is holding a large, mechanical-looking sword with a black and silver design. The sword has a long, cylindrical handle and a sharp blade. In the background, there is a floating blue crystal or gemstone, and the character appears to be in a snowy or cold environment, as suggested by the white, snowy landscape and the character's attire. The overall style is vibrant and detailed, with a focus on the character's expression and the mechanical elements of the sword.\"]\n",
      "2025-01-27 19:18:42 | INFO | stdout | [\"The image is an illustration of an anime-style character. The character has long, flowing blue hair and is wearing a beige coat with a high collar and a black dress underneath. The coat has a brownish hue and features a large, decorative button on the front. The character is holding a large, mechanical-looking sword with a black and silver design. The sword has a long, cylindrical handle and a sharp blade. In the background, there is a floating blue crystal or gemstone, and the character appears to be in a snowy or cold environment, as suggested by the white, snowy landscape and the character's attire. The overall style is vibrant and detailed, with a focus on the character's expression and the mechanical elements of the sword.\"]\n",
      "2025-01-27 19:20:20 | INFO | stdout | [\"The image is an illustration of an anime-style character. The character has long, flowing blue hair and is wearing a beige coat with a high collar and a black dress underneath. The coat has a brownish hue and features a large, decorative button on the front. The character is holding a large, mechanical-looking sword with a black and silver design. The sword has a long, cylindrical handle and a sharp blade. In the background, there is a floating blue crystal or gemstone, and the character appears to be in a snowy or cold environment, as suggested by the white, snowy landscape and the character's attire. The overall style is vibrant and detailed, with a focus on the character's expression and the mechanical elements of the sword.\"]\n",
      "STAGE:2025-01-27 19:23:17 77655:77655 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-01-27 19:23:31 77655:77655 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-01-27 19:23:31 77655:77655 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "STAGE:2025-01-27 19:26:10 77655:77655 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "2025-01-27 19:27:53 | INFO | stdout | [INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "2025-01-27 19:27:53 | INFO | stdout | [INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "2025-01-27 19:27:53 | INFO | stdout | [INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "2025-01-27 19:27:53 | INFO | stdout | [INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "2025-01-27 19:29:19 | INFO | stdout | [INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "2025-01-27 19:29:19 | INFO | stdout | [INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "2025-01-27 19:29:19 | INFO | stdout | [INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "2025-01-27 19:29:19 | INFO | stdout | [INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "2025-01-27 19:38:46 | INFO | stdout | [INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "2025-01-27 19:38:46 | INFO | stdout | [INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "2025-01-27 19:38:46 | INFO | stdout | [INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "2025-01-27 19:38:46 | INFO | stdout | [INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "2025-01-27 19:41:31 | INFO | stdout | [INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "2025-01-27 19:41:31 | INFO | stdout | [INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "2025-01-27 19:41:31 | INFO | stdout | [INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "2025-01-27 19:41:31 | INFO | stdout | [INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "2025-01-27 19:42:20 | INFO | stdout | [INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "2025-01-27 19:42:20 | INFO | stdout | [INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "2025-01-27 19:42:20 | INFO | stdout | [INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "2025-01-27 19:42:20 | INFO | stdout | [INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "2025-01-27 19:45:58 | INFO | stdout | [INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "2025-01-27 19:45:58 | INFO | stdout | [INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "2025-01-27 19:45:58 | INFO | stdout | [INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "2025-01-27 19:45:58 | INFO | stdout | [INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "2025-01-27 19:47:38 | INFO | stdout | [INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "2025-01-27 19:47:38 | INFO | stdout | [INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "2025-01-27 19:47:38 | INFO | stdout | [INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "2025-01-27 19:47:38 | INFO | stdout | [INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "2025-01-27 20:29:16 | INFO | stdout | [INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "2025-01-27 20:29:16 | INFO | stdout | [INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "2025-01-27 20:29:16 | INFO | stdout | [INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "2025-01-27 20:29:16 | INFO | stdout | [INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "2025-01-27 20:29:22 | INFO | stdout | [INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "2025-01-27 20:29:22 | INFO | stdout | [INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "2025-01-27 20:29:22 | INFO | stdout | [INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "2025-01-27 20:29:22 | INFO | stdout | [INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import io\n",
    "from typing import Dict\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor, TextIteratorStreamer\n",
    "import sys\n",
    "from threading import Thread\n",
    "\n",
    "from tool_server.tf_eval.utils.utils import *\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model in half-precision on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"/mnt/petrelfs/songmingyang/songmingyang/model/tool-augment/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"/mnt/petrelfs/songmingyang/songmingyang/model/tool-augment/Qwen2-VL-7B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image\n",
    "url = \"/mnt/petrelfs/songmingyang/code/tools/test_imgs/roxy.jpeg\"\n",
    "image = Image.open(url)\n",
    "image_base64=pil_to_base64(image)\n",
    "image_base64 = f\"data:image/jpeg;base64,{image_base64}\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "processor.tokenizer.padding_side='left'\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=2048)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The image is an illustration of an anime-style character. The character has long, flowing blue hair and is wearing a beige coat with a high collar and a black dress underneath. The coat has a brownish hue and features a large, decorative button on the front. The character is holding a large, mechanical-looking sword with a black and silver design. The sword has a long, cylindrical handle and a sharp blade. In the background, there is a floating blue crystal or gemstone, and the character appears to be in a snowy or cold environment, as suggested by the white, snowy landscape and the character's attire. The overall style is vibrant and detailed, with a focus on the character's expression and the mechanical elements of the sword.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't disable Kineto profiler when it's not running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProfilerActivity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCPU\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProfilerActivity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCUDA\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_trace_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensorboard_trace_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./log\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_stack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecord_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_flops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 开启 FLOPS 统计\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprof\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerated_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/smoe/lib/python3.11/site-packages/torch/profiler/profiler.py:605\u001b[0m, in \u001b[0;36mprofile.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/smoe/lib/python3.11/site-packages/torch/profiler/profiler.py:613\u001b[0m, in \u001b[0;36mprofile.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transit_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mProfilerAction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNONE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_steps:\n\u001b[1;32m    615\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_rec_fn \u001b[38;5;241m=\u001b[39m prof\u001b[38;5;241m.\u001b[39mrecord_function(\n\u001b[1;32m    616\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProfilerStep#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_num)\n\u001b[1;32m    617\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/smoe/lib/python3.11/site-packages/torch/profiler/profiler.py:651\u001b[0m, in \u001b[0;36mprofile._transit_action\u001b[0;34m(self, prev_action, current_action)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_list:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m action_list:\n\u001b[0;32m--> 651\u001b[0m         \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/smoe/lib/python3.11/site-packages/torch/profiler/profiler.py:126\u001b[0m, in \u001b[0;36m_KinetoProfile.prepare_trace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler \u001b[38;5;241m=\u001b[39m prof\u001b[38;5;241m.\u001b[39mprofile(\n\u001b[1;32m    114\u001b[0m         use_cuda\u001b[38;5;241m=\u001b[39m(ProfilerActivity\u001b[38;5;241m.\u001b[39mCUDA \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivities),\n\u001b[1;32m    115\u001b[0m         use_cpu\u001b[38;5;241m=\u001b[39m(ProfilerActivity\u001b[38;5;241m.\u001b[39mCPU \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivities),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m         experimental_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_config,\n\u001b[1;32m    125\u001b[0m     )\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/smoe/lib/python3.11/site-packages/torch/autograd/profiler.py:311\u001b[0m, in \u001b[0;36mprofile._prepare_trace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     \u001b[43m_prepare_profiler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkineto_activities\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't disable Kineto profiler when it's not running"
     ]
    }
   ],
   "source": [
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA\n",
    "    ],\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./log\"),\n",
    "    with_stack=True,\n",
    "    record_shapes=True,\n",
    "    with_flops=True,  # 开启 FLOPS 统计\n",
    ") as prof:\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=2048)\n",
    "# print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "# prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvcore.nn import FlopCountAnalysis, parameter_count\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops = FlopCountAnalysis(model, input_tensor)\n",
    "params = parameter_count(model)\n",
    "\n",
    "print(f\"FLOPS: {flops.total()}\")  # 总 FLOPS\n",
    "print(f\"Params: {params}\")        # 参数数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "pixel_values = inputs[\"pixel_values\"]\n",
    "image_grid_thw = inputs[\"image_grid_thw\"]\n",
    "\n",
    "# flops, params = profile(model, inputs=(input,))\n",
    "flops, params = profile(model, inputs=(input_ids, attention_mask, None,\n",
    "                                       None,None,None,None,None,None,None,pixel_values,None,image_grid_thw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95443525828608.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8135, 1.9303, 1.4486,  ..., 2.1459, 2.1459, 2.1459],\n",
       "        [1.9303, 1.9303, 1.9303,  ..., 2.1459, 2.1459, 2.1459],\n",
       "        [1.9303, 1.9011, 1.9303,  ..., 2.1459, 2.1459, 2.1459],\n",
       "        ...,\n",
       "        [1.9303, 1.9303, 1.9303,  ..., 2.1459, 2.1459, 2.1459],\n",
       "        [1.9303, 1.9303, 1.9303,  ..., 2.1459, 2.1459, 2.1459],\n",
       "        [1.9303, 1.9303, 1.9303,  ..., 2.1459, 2.1459, 2.1459]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7746173952.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smoe",
   "language": "python",
   "name": "smoe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
