{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from mr_eval.utils.utils import *\n",
    "import random\n",
    "import os,sys\n",
    "import os,sys\n",
    "sys.path.append(\"/mnt/petrelfs/songmingyang/code/mm/robustLMM/robustlmm/model_inference/llama_inference/inference\")\n",
    "from prompts import prompt_dict\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "prompt_dict = prompt_dict[\"llava_caption_to_conversation\"]\n",
    "\n",
    "def convert_to_human_readable_size(num):\n",
    "    if num / 1e27 > 1:\n",
    "        return f\"{num / 1e27:.2f} R\"\n",
    "    elif num / 1e24 > 1:\n",
    "        return f\"{num / 1e24:.2f} Y\"\n",
    "    elif num / 1e21 > 1:\n",
    "        return f\"{num / 1e21:.2f} Z\"\n",
    "    elif num / 1e18 > 1:\n",
    "        return f\"{num / 1e18:.2f} E\"\n",
    "    elif num / 1e15 > 1:\n",
    "        return f\"{num / 1e15:.2f} P\"\n",
    "    elif num / 1e12 > 1:\n",
    "        return f\"{num / 1e12:.2f} T\"\n",
    "    elif num / 1e9 > 1:\n",
    "        return f\"{num / 1e9:.2f} B\"\n",
    "    elif num / 1e6 > 1:\n",
    "        return f\"{num / 1e6:.2f} M\"\n",
    "    elif num / 1e3 > 1:\n",
    "        return f\"{num / 1e3:.2f} K\"\n",
    "    else:\n",
    "        return f\"{num}\"\n",
    "    \n",
    "def get_flops(\n",
    "    num_layers,\n",
    "    hidden_size,\n",
    "    num_heads,\n",
    "    num_key_value_heads,\n",
    "    vocab_size,\n",
    "    seq_len,\n",
    "    ffn_hidden_size,\n",
    "    batch_size=1,\n",
    "):\n",
    "    \"\"\"Counts flops in an decoder-only model\n",
    "    Args:\n",
    "        num_layers: number of decoder layers\n",
    "        hidden_size: hidden size of the model\n",
    "        num_heads: number of heads in the model\n",
    "        num_key_value_heads: number of key/value heads in the model\n",
    "        ffn_hidden_size: hidden size of the FFN\n",
    "        vocab_size: size of the vocabulary\n",
    "        seq_len: sequence length of the decoder\n",
    "        batch_size: batch size\n",
    "    Returns:\n",
    "        model_flops: flops in the model (should be independent of the hardware and model implementation)\n",
    "        hardware_flops: flops in the hardware (actual flops performed on the hardware). Check 6.3 in https://arxiv.org/pdf/2205.05198.pdf\n",
    "    \"\"\"\n",
    "    if num_key_value_heads is None:\n",
    "        num_key_value_heads = num_heads\n",
    "    hidden_size_per_head = hidden_size // num_heads\n",
    "    # In the following we mark the reduced dimension with parentheses\n",
    "    # decoder\n",
    "    # self attention\n",
    "    ## qkv projection\n",
    "    decoder_qkv_proj_flops_fwd = (\n",
    "        2 * num_layers * batch_size * seq_len * (hidden_size) * num_heads * hidden_size_per_head\n",
    "        + 2 * num_layers * batch_size * seq_len * (hidden_size) * 2 * num_key_value_heads * hidden_size_per_head\n",
    "    )\n",
    "    ## qk logits\n",
    "    decoder_qk_logits_flops_fwd = 2 * num_layers * batch_size * num_heads * seq_len * (hidden_size_per_head) * seq_len\n",
    "    ## v logits\n",
    "    decoder_v_logits_flops_fwd = 2 * num_layers * batch_size * num_heads * seq_len * (seq_len) * hidden_size_per_head\n",
    "    ## attn out\n",
    "    decoder_attn_out_flops_fwd = (\n",
    "        2 * num_layers * batch_size * num_heads * seq_len * (hidden_size_per_head) * hidden_size\n",
    "    )\n",
    "    # FF\n",
    "    ## 1st layer\n",
    "    decoder_ffn_1_flops_fwd = 4 * num_layers * batch_size * seq_len * (hidden_size) * ffn_hidden_size\n",
    "    ## 2nd layer\n",
    "    decoder_ffn_2_flops_fwd = 2 * num_layers * batch_size * seq_len * (ffn_hidden_size) * hidden_size\n",
    "\n",
    "    decoder_flops_fwd = (\n",
    "        decoder_qkv_proj_flops_fwd\n",
    "        + decoder_qk_logits_flops_fwd\n",
    "        + decoder_v_logits_flops_fwd\n",
    "        + decoder_attn_out_flops_fwd\n",
    "        + decoder_ffn_1_flops_fwd\n",
    "        + decoder_ffn_2_flops_fwd\n",
    "    )\n",
    "\n",
    "    # lm head\n",
    "    lm_head_flops_fwd = 2 * batch_size * seq_len * (hidden_size) * vocab_size\n",
    "\n",
    "    # the bwd pass requires double the flops in case of matmuls to calculate the gradients with respect to\n",
    "    # both input and weight tensors\n",
    "    model_flops = 3 * (decoder_flops_fwd + lm_head_flops_fwd)  # 1 for fwd + 2 for bwd\n",
    "\n",
    "    hardware_flops = model_flops  # TODO: This is a placeholder for now\n",
    "\n",
    "    return model_flops, hardware_flops\n",
    "\n",
    "\n",
    "\n",
    "input_data = \"/mnt/petrelfs/songmingyang/songmingyang/data/llava_train/LLaVA-Instruct-150K/control/llavaft_control_llava13b_caps.jsonl\"\n",
    "input_data = process_jsonl(input_data)\n",
    "\n",
    "output_data = \"/mnt/petrelfs/songmingyang/songmingyang/data/llava_train/LLaVA-Instruct-150K/control/llavaft_control_llava13b_convs.jsonl\"\n",
    "output_data = process_jsonl(output_data)\n",
    "\n",
    "output_length_dict = {item[\"id\"]: len(item[\"origin_output\"]) for item in output_data}\n",
    "\n",
    "input_data = random.sample(input_data, 1000)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/petrelfs/songmingyang/quxiaoye/models/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "\n",
    "\n",
    "# Replace with your LLaMA model path or Hugging Face hub name\n",
    "model_name = \"/mnt/petrelfs/songmingyang/quxiaoye/models/Meta-Llama-3-70B-Instruct-back\"\n",
    "\n",
    "# Load the model configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# Access key parameters\n",
    "num_layers = config.num_hidden_layers\n",
    "hidden_size = config.hidden_size\n",
    "num_heads = config.num_attention_heads\n",
    "num_key_value_heads = getattr(config, \"num_key_value_heads\", None)  # Might not exist in all models\n",
    "vocab_size = config.vocab_size\n",
    "seq_len = config.max_position_embeddings\n",
    "ffn_hidden_size = getattr(config, \"intermediate_size\", None)  # FFN hidden size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 1635, Target length: 1938\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt_dict[\"system\"]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_dict[\"fewshot\"][0][0]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": prompt_dict[\"fewshot\"][0][1]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_dict[\"fewshot\"][1][0]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": prompt_dict[\"fewshot\"][1][1]\n",
    "    },\n",
    "]\n",
    "target_input_data = input_data[3]\n",
    "\n",
    "messages2 = deepcopy(messages)\n",
    "messages2.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": target_input_data[\"caption\"]\n",
    "    },)\n",
    "\n",
    "target_length = output_length_dict[target_input_data[\"id\"]]\n",
    "inputs = tokenizer.apply_chat_template(messages2, tokenize=True)\n",
    "\n",
    "print(f\"Input length: {len(inputs)}, Target length: {target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1635"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 122.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total flops: 1.75 Z\n",
      "Real flops: 145.94 Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "target_convs = []\n",
    "all_flops = 0\n",
    "output_path = \"./llama3_flops.jsonl\"\n",
    "for idx,item in enumerate(tqdm(input_data)):\n",
    "    current_message = deepcopy(messages)\n",
    "    current_message.append({\"role\": \"user\",\"content\":item[\"caption\"]})\n",
    "    inputs = tokenizer.apply_chat_template(current_message, tokenize=True)\n",
    "    start_length = len(inputs)\n",
    "    item_id = item[\"id\"]\n",
    "    target_length = output_length_dict.get(item_id, 1000)\n",
    "    total_flops = 0\n",
    "    for i in range(start_length, start_length+target_length):\n",
    "        seq_len = i\n",
    "        flops = get_flops(num_layers, hidden_size, num_heads, num_key_value_heads, vocab_size, seq_len, \n",
    "                          ffn_hidden_size, batch_size=1)\n",
    "        total_flops += flops[0]\n",
    "    append_jsonl({\"id\": item_id, \"flops\": total_flops}, output_path)\n",
    "    all_flops += total_flops\n",
    "\n",
    "print(f\"Total flops: {convert_to_human_readable_size(all_flops)}\")\n",
    "    \n",
    "balance = 581745\n",
    "aug = 665298\n",
    "\n",
    "real_flops = all_flops / 1000 * (aug - balance)\n",
    "print(f\"Real flops: {convert_to_human_readable_size(real_flops)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.3553"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance = 581745\n",
    "aug = 665298\n",
    "100*(aug-balance)/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83553"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug-balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.279999999999998"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8.36+8.26+1.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mr_eval.utils.utils import *\n",
    "input_data = \"/mnt/petrelfs/songmingyang/songmingyang/data/llava_train/sharegpt4v_train/pt/share-captioner_coco_lcs_sam_1246k_1107.json\"\n",
    "input_data = load_json_file(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1246901"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168639"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_data = \"/mnt/petrelfs/songmingyang/songmingyang/data/llava_train/sharegpt4v_train/pt/reformed_data/dinoobj_toc/share_meta_dinoobj_all_p0_alpha_1.json\"\n",
    "origin_data = \"/mnt/petrelfs/songmingyang/songmingyang/data/llava_train/sharegpt4v_train/pt/reformed_data/dinoobj_toc/share_meta_dinoobj_all_p0_alpha_1_adjust_threshold.json\"\n",
    "origin_data = load_json_file(origin_data)\n",
    "len(origin_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.8262"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance = 1168639\n",
    "aug = 1246901\n",
    "100*(aug-balance)/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.96"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7.83 + 1.55 + 7.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78262"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug-balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
