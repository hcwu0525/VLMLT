{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mhr.utils.utils import load_json_file,process_jsonl\n",
    "\n",
    "\n",
    "def read_results_from_file(file_path):\n",
    "    if os.path.isdir(file_path):\n",
    "        file_path = os.path.join(file_path, \"results.json\")\n",
    "    result_dict = load_json_file(file_path)[\"results\"]\n",
    "    return result_dict\n",
    "\n",
    "# baseline_file = \"/mnt/petrelfs/songmingyang/code/tools/lmms-eval/scripts/logs/llava/robustlmm/dr_algo/0601_2205_eval_all_llava_baseline\"\n",
    "# ptnft0_file = \"/mnt/petrelfs/songmingyang/code/tools/lmms-eval/scripts/logs/llava/robustlmm/dr_algo/0604_1109_eval_all_dr_algo_pt0_ft0\"\n",
    "# ptnft1_file = \"/mnt/petrelfs/songmingyang/code/tools/lmms-eval/scripts/logs/llava/robustlmm/dr_algo/0603_1918_eval_all_dr_algo_pt0_ft1\"\n",
    "# ptnft2_file = \"/mnt/petrelfs/songmingyang/code/tools/lmms-eval/scripts/logs/llava/robustlmm/dr_algo/0604_2258_eval_all_dr_algo_pt0_ft2\"\n",
    "share4v_base_file=\"/mnt/petrelfs/songmingyang/code/tools/lmms-eval/scripts/logs/robustlmm/sharegpt4v/0723_0003_share4v_baseline_sharegpt4v_model_args_f37f0e\"\n",
    "baseline_file = \"/mnt/petrelfs/songmingyang/code/tools/lmms-eval/scripts/logs/robustlmm/dr_algo/0605_2135_eval_all_llava_baseline_llava_model_args_ca10f3\"\n",
    "mplug_owl_chat_file=\"/mnt/petrelfs/songmingyang/code/tools/lmms-eval/scripts/logs/robustlmm/other_models/0806_2242_mplug_owl_mplug_owl_model_args_2b40be\"\n",
    "\n",
    "baseline_data = read_results_from_file(baseline_file)\n",
    "share4v_data = read_results_from_file(share4v_base_file)\n",
    "mplug_owl_chat_data = read_results_from_file(mplug_owl_chat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric_list = [\"vqav2\",\"gqa\",\"vizwiz_vqa\",\"scienceqa_img\",\"pope\",\"mme\",\"seedbench\",\"seedbench-2\",\"refcoco\",\"refcoco+\",\"mmmu\"]\n",
    "metric1_list = [\"vqav2\",\"textvqa\",\"ok_vqa\",\"gqa\",\"vizwiz_vqa\",\"scienceqa\",\"scienceqa_img\",\"refcoco\",\"refcoco+\",\"flickr30k\",\"pope\",\"seedbench\",\"mmmu\"]\n",
    "metric2_list = [\"pope\",\"mme\",\"seedbench\",\"seedbench-2\",\"mmbench_en_dev\",\"mmbench_cn_dev\",\"llava_bench_coco\",\"llava_in_the_wild\",\"mmmu\"]\n",
    "metric2_list = [\"pope\",\"mme\",\"seedbench\",\"seedbench-2\",\"mmmu\"]\n",
    "metric_dict = {\n",
    "    \"vqav2\": [\"exact_match,none\"],\n",
    "    \"textvqa\": [\"exact_match,none\"],\n",
    "    \"gqa\":[\"exact_match,none\"],\n",
    "    \"mme\":[\"mme_percetion_score,none\",\"mme_cognition_score,none\"],\n",
    "    \"mmmu\":[\"mmmu_acc,none\"],\n",
    "    \"pope\":[\"pope_accuracy,none\"],\n",
    "    \"refcoco\":[\"refcoco_CIDEr,none\"],\n",
    "    \"refcoco+\":[\"refcoco_CIDEr,none\"],\n",
    "    \"scienceqa\":[\"exact_match,none\"],\n",
    "    \"scienceqa_img\":[\"exact_match,none\"],\n",
    "    \"seedbench\":[\"seed_all,none\"],\n",
    "    \"seedbench-2\":[\"seed_all,none\"],\n",
    "    \"vizwiz_vqa\":[\"exact_match,none\"],\n",
    "    \"flickr30k\":[\"flickr_CIDEr,none\"],\n",
    "    \"hallusion_bench_image\":[\"aAcc,none\"],\n",
    "    \"llava_bench_coco\":[\"gpt_eval_llava_all,none\"],\n",
    "    \"llava_in_the_wild\":[\"gpt_eval_llava_all,none\"],\n",
    "    \"mmbench_cn_dev\":[\"gpt_eval_score,none\"],\n",
    "    \"mmbench_en_dev\":[\"gpt_eval_score,none\"],\n",
    "    \"mmvet\":[\"gpt_eval_score,none\"],\n",
    "    \"ok_vqa\":[\"exact_match,none\"],\n",
    "    \"textvqa\":[\"exact_match,none\"],\n",
    "}\n",
    "llava_paper_dict=dict(\n",
    "    # vqav2=78.5,\n",
    "    gqa=62.0,\n",
    "    vizwiz_vqa=50.0,\n",
    "    scienceqa_img=66.8,\n",
    "    # textvqa=58.2,\n",
    "    pope=85.9,\n",
    "    mme=1510.7,\n",
    "    mmbench_en_dev=64.3,\n",
    "    mmbench_cn_dev=58.3,\n",
    "    seedbench=58.6,\n",
    "    llava_in_the_wild=63.4,\n",
    "    mmvet=30.5,\n",
    ")\n",
    "\n",
    "\n",
    "ft0_train_data_length = 561.6*1000\n",
    "ft1_train_data_length = 242.2*1000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "method_dict = {\n",
    "    \"LLaVA 1.5\":{\"data\":baseline_data,},\n",
    "    \"ShareGPT4V\":{\"data\":share4v_data,},\n",
    "    \"mPLUG-Owl\":{\"data\":mplug_owl_chat_data,},\n",
    "}\n",
    "method_list = list(method_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_to_list(method_list,method_dict,metric_list,metric_dict):\n",
    "    res = {}\n",
    "    for method in method_list:\n",
    "        data = method_dict[method][\"data\"]\n",
    "        method_res = []\n",
    "        for metric in metric_list:\n",
    "            for metric_idx,metric_item in enumerate(metric_dict[metric]):\n",
    "                if method == \"baseline\" and metric_idx == 0 and llava_paper_dict.get(metric,-1) != -1:\n",
    "                    metric_res = llava_paper_dict[metric]\n",
    "                else:\n",
    "                    metric_res = data[metric].get(metric_item,-1)\n",
    "                    if metric not in [\"mme\",\"mmbench_cn_dev\",\"mmbench_en_dev\",\"llava_bench_coco\",\"llava_in_the_wild\"]:\n",
    "                        metric_res *= 100\n",
    "                method_res.append(metric_res)\n",
    "        res[method] = method_res\n",
    "    return res \n",
    "\n",
    "def form_latex_str(res,method_dict):\n",
    "    res_str = \"\"\n",
    "    method_list = list(res.keys())\n",
    "    res_dict = {method:f\"{method} \" for method in method_list}\n",
    "    avg_dict = {method:[] for method in method_list}\n",
    "    for i in range(len(res[method_list[0]])):\n",
    "        compare_list = sorted([res[method][i] for method in method_list])\n",
    "        for method in method_list:\n",
    "            if res[method][i] == compare_list[-1]:\n",
    "                res_dict[method] += f\"& \\\\textbf{{{res[method][i]:.1f}}}\"\n",
    "            elif res[method][i] == compare_list[-2]:\n",
    "                res_dict[method] += f\"& \\\\underline{{{res[method][i]:.1f}}}\"\n",
    "            else:\n",
    "                res_dict[method] += f\"& {res[method][i]:.1f}\"\n",
    "            avg_dict[method].append(res[method][i])\n",
    "    avg_dict = {method:f\" & {sum(avg_dict[method])/len(avg_dict[method]):.1f}\" for method in method_list}\n",
    "    for method in method_list:\n",
    "        res_str += res_dict[method] + avg_dict[method] + \"\\\\\\\\ \\n\"\n",
    "    return res_str\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA 1.5 & \\underline{76.6}& \\underline{46.0}& \\underline{53.2}& \\underline{61.9}& \\underline{54.2}& \\underline{70.4}& \\textbf{69.3}& \\underline{29.4}& \\underline{28.5}& \\underline{74.9}& \\textbf{86.9}& \\underline{60.6}& \\textbf{35.3} & 57.5\\\\ \n",
      "ShareGPT4V & \\textbf{78.6}& \\textbf{50.2}& \\textbf{54.0}& \\textbf{63.3}& \\textbf{59.1}& \\textbf{71.4}& \\underline{68.9}& \\textbf{37.6}& \\textbf{34.3}& \\textbf{78.5}& \\underline{86.8}& \\textbf{63.2}& \\underline{35.1} & 60.1\\\\ \n",
      "mPLUG-Owl & 27.6& 6.3& 2.4& 28.2& 50.6& 0.1& 0.0& 0.0& 0.0& 2.2& 0.0& 17.6& 24.9 & 12.3\\\\ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = get_results_to_list(method_list,method_dict,metric1_list,metric_dict)\n",
    "res_str = form_latex_str(res,method_dict)\n",
    "print(res_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline & 558.0K & 665.0K & 85.9& \\underline{1510.7}& \\textbf{349.6}& 58.6& \\textbf{58.0}& 35.3 & 349.7\\\\ \n",
      "all+$\\alpha=1.0$ & 558.0K & 581.7K & \\textbf{87.2}& 1470.6& 329.6& 61.0& 57.2& 34.8 & 340.1\\\\ \n",
      "toc+$\\alpha=1.0$ & 558.0K & 561.5K & 86.6& 1510.5& 316.8& 60.6& 57.1& 35.2 & 344.5\\\\ \n",
      "all+$\\alpha=0.8$+aug & 558.0K & 664.3K & 86.9& 1505.0& 300.7& 60.9& 57.1& 35.2 & 341.0\\\\ \n",
      "toc+$\\alpha=0.8$+aug & 558.0K & 665.4K & \\underline{87.1}& 1480.6& \\underline{347.9}& \\underline{61.0}& 57.2& \\underline{36.0} & 345.0\\\\ \n",
      "all+$\\alpha=1.0$+aug & 558.0K & 665.7K & 86.9& \\textbf{1511.3}& 291.8& \\textbf{61.3}& \\underline{57.4}& \\textbf{36.3} & 340.8\\\\ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = get_results_to_list(method_list,method_dict,metric2_list,metric_dict)\n",
    "res_str = form_latex_str(res,method_dict)\n",
    "print(res_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one aspect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
