{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mhr.utils.utils import *\n",
    "\n",
    "benchmark_statistic_file= \"/mnt/petrelfs/songmingyang/songmingyang/data/mm/annotation/vqav2/statistics/val14_statistics.jsonl\"\n",
    "\n",
    "threshold_dict = {'object': 304, 'token': 120, 'co_occurrence': 24, 'what_word': 4895}\n",
    "reverse_index_file_dict = {\n",
    "    'object': '/mnt/petrelfs/songmingyang/songmingyang/data/llava_train/LLaVA-Instruct-150K/reverse_index/llava_v1_5_mix665k_dino_stat_reverse_index.jsonl',\n",
    "    'token': '/mnt/petrelfs/songmingyang/songmingyang/data/llava_train/LLaVA-Instruct-150K/reverse_index/llava_v1_5_mix665k_token_reverse_index.jsonl',\n",
    "    'co_occurrence': '/mnt/petrelfs/songmingyang/songmingyang/data/llava_train/LLaVA-Instruct-150K/reverse_index/llava_v1_5_mix665k_co_occurrence_reverse_index.jsonl',\n",
    "    'what_word': '/mnt/petrelfs/songmingyang/songmingyang/data/llava_train/LLaVA-Instruct-150K/reverse_index/llava_v1_5_mix665k_what_word_reverse_index.jsonl'\n",
    "}\n",
    "\n",
    "token_input_file,object_input_file,co_occurrence_input_file,what_word_input_file = reverse_index_file_dict['token'],reverse_index_file_dict['object'],reverse_index_file_dict['co_occurrence'],reverse_index_file_dict['what_word']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_to_dict(obj,dictionary):\n",
    "    return dictionary.get(obj,-1)\n",
    "\n",
    "def get_90_index(data,sum,ratio=0.9):\n",
    "    sum_90 = sum*ratio\n",
    "    sum_temp = 0\n",
    "    for i in range(len(data)):\n",
    "        sum_temp += data[i][1]\n",
    "        if sum_temp >= sum_90:\n",
    "            return i\n",
    "        \n",
    "def get_scores(data,dictionary,threshold):\n",
    "        scores = []\n",
    "        metadata = []\n",
    "        for item in data:\n",
    "            score = lookup_to_dict(item,dictionary)\n",
    "            if score >= 0:\n",
    "                scores.append(score)\n",
    "                metadata.append({\"object\":item,\"score\":score})\n",
    "        avg_score = sum(scores)/len(scores) if len(scores) > 0 else 0\n",
    "        max_score = max(scores) if len(scores) > 0 else 0\n",
    "        avg_pass = avg_score >= threshold\n",
    "        least_pass = max_score >= threshold\n",
    "        return scores,metadata,avg_pass,least_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_data = process_jsonl(benchmark_statistic_file)\n",
    "\n",
    "for item in statistic_data:\n",
    "    item[\"statistics\"] = item[\"statistic\"]\n",
    "\n",
    "\n",
    "token_data = process_jsonl(token_input_file)\n",
    "object_data = process_jsonl(object_input_file)\n",
    "co_occurrence_data = process_jsonl(co_occurrence_input_file)\n",
    "what_word_data = process_jsonl(what_word_input_file)\n",
    "\n",
    "token_data.sort(key=lambda x: len(x[\"ids\"]), reverse=True)\n",
    "object_data.sort(key=lambda x: len(x[\"ids\"]), reverse=True)\n",
    "co_occurrence_data.sort(key=lambda x: len(x[\"ids\"]), reverse=True)\n",
    "what_word_data.sort(key=lambda x: len(x[\"ids\"]), reverse=True)\n",
    "\n",
    "token_data = [(x[\"object\"],len(x[\"ids\"])) for x in token_data]\n",
    "object_data = [(x[\"object\"],len(x[\"ids\"])) for x in object_data]\n",
    "co_occurrence_data = [(x[\"object\"],len(x[\"ids\"])) for x in co_occurrence_data]\n",
    "what_word_data = [(x[\"object\"],len(x[\"ids\"])) for x in what_word_data]\n",
    "\n",
    "token_sum = sum([x[1] for x in token_data])\n",
    "object_sum = sum([x[1] for x in object_data])\n",
    "co_occurrence_sum = sum([x[1] for x in co_occurrence_data])\n",
    "what_word_sum = sum([x[1] for x in what_word_data])\n",
    "\n",
    "token_threshold,object_threshold,co_occurrence_threshold,what_word_threshold = [0.9]*4 \n",
    "token_90_loc = get_90_index(token_data,token_sum,token_threshold)\n",
    "object_90_loc = get_90_index(object_data,object_sum,object_threshold)\n",
    "co_occurrence_90_loc = get_90_index(co_occurrence_data,co_occurrence_sum,co_occurrence_threshold)\n",
    "what_word_90_loc = get_90_index(what_word_data,what_word_sum,what_word_threshold)\n",
    "\n",
    "token_dict = {x[0]:idx for idx,x in enumerate(token_data)}\n",
    "object_dict = {x[0]:idx for idx,x in enumerate(object_data)}\n",
    "co_occurrence_dict = {x[0]:idx for idx,x in enumerate(co_occurrence_data)}\n",
    "what_word_dict = {x[0]:idx for idx,x in enumerate(what_word_data)}\n",
    "for item in statistic_data:\n",
    "        tokens = item[\"statistics\"][\"token\"]\n",
    "        objects = item[\"statistics\"][\"object\"]\n",
    "        co_occurrences = item[\"statistics\"][\"co_occurrence\"]\n",
    "        what_words = item[\"statistics\"][\"what_word\"]\n",
    "        \n",
    "        token_scores,token_metadata,token_avg_pass,token_least_pass = get_scores(tokens,token_dict,token_90_loc)\n",
    "        object_scores,object_metadata,object_avg_pass,object_least_pass = get_scores(objects,object_dict,object_90_loc)\n",
    "        co_occurrence_scores,co_occurrence_metadata,co_occurrence_avg_pass,co_occurrence_least_pass = get_scores(co_occurrences,co_occurrence_dict,co_occurrence_90_loc)\n",
    "        what_word_scores,what_word_metadata,what_word_avg_pass,what_word_least_pass = get_scores(what_words,what_word_dict,what_word_90_loc)\n",
    "        distribution_item = dict(token=dict(scores=token_scores,metadata=token_metadata,avg_pass=token_avg_pass,least_pass=token_least_pass),\n",
    "                                object=dict(scores=object_scores,metadata=object_metadata,avg_pass=object_avg_pass,least_pass=object_least_pass),\n",
    "                                co_occurrence=dict(scores=co_occurrence_scores,metadata=co_occurrence_metadata,avg_pass=co_occurrence_avg_pass,least_pass=co_occurrence_least_pass),\n",
    "                                what_word=dict(scores=what_word_scores,metadata=what_word_metadata,avg_pass=what_word_avg_pass,least_pass=what_word_least_pass))\n",
    "        item[\"distribution\"] = distribution_item\n",
    "\n",
    "token_scores,token_metadata,token_avg_pass,token_least_pass = get_scores(tokens,token_dict,token_90_loc)\n",
    "\n",
    "head_tail_pass_key = \"avg_pass\"\n",
    "statistic_id_key = \"question_id\"\n",
    "\n",
    "token_pass_cnt = 0\n",
    "object_pass_cnt = 0\n",
    "co_occurrence_pass_cnt = 0\n",
    "what_word_pass_cnt = 0\n",
    "all_pass_cnt = 0\n",
    "pass_key = head_tail_pass_key\n",
    "pass_ids = []\n",
    "not_pass_ids = []\n",
    "for item in statistic_data:\n",
    "    token_pass,object_pass,co_occurrence_pass,what_word_pass = 0,0,0,0\n",
    "    if item[\"distribution\"][\"token\"][pass_key]:\n",
    "        token_pass = 1\n",
    "        token_pass_cnt += 1\n",
    "    if item[\"distribution\"][\"object\"][pass_key]:\n",
    "        object_pass = 1\n",
    "        object_pass_cnt += 1\n",
    "    if item[\"distribution\"][\"co_occurrence\"][pass_key]:\n",
    "        co_occurrence_pass_cnt += 1\n",
    "        co_occurrence_pass = 1\n",
    "    if item[\"distribution\"][\"what_word\"][pass_key]:\n",
    "        what_word_pass_cnt += 1\n",
    "        what_word_pass = 1\n",
    "    if token_pass + object_pass + co_occurrence_pass + what_word_pass >= 1:\n",
    "        all_pass_cnt += 1\n",
    "        pass_ids.append(item[statistic_id_key])\n",
    "    else:\n",
    "        not_pass_ids.append(item[statistic_id_key])\n",
    "pass_ids = pass_ids\n",
    "not_pass_ids = not_pass_ids\n",
    "tail_dict = {i:1 for i in pass_ids}\n",
    "head_dict = {i:1 for i in not_pass_ids}\n",
    "all_pass_cnt = all_pass_cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47624"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tail_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_statistic_data(self):\n",
    "    assert isinstance(statistic_files,list)\n",
    "    if len(statistic_files) == 1:\n",
    "        if statistic_files[0].endswith(\".jsonl\"):\n",
    "            statistic_data = process_jsonl(statistic_files[0])\n",
    "        elif statistic_files[0].endswith(\".json\"):\n",
    "            statistic_data = load_json_file(statistic_files[0])\n",
    "        if statistic_data[0].get(\"statistic\",None) is not None:\n",
    "            for item in statistic_data:\n",
    "                item[\"statistics\"] = item[\"statistic\"]\n",
    "        elif statistic_data[0].get(\"statistics\",None) is None:\n",
    "            raise ValueError(\"The statistic data should contain 'statistics' or 'statistic' key\")\n",
    "    else:\n",
    "        token_data = process_jsonl(statistic_files[0])\n",
    "        object_data = process_jsonl(statistic_files[1])\n",
    "        what_word_data = process_jsonl(statistic_files[2])\n",
    "        \n",
    "        object_dict = {item[statistic_id_key]:str2list(item[statistic_object_keys[1]]) for item in object_data}\n",
    "        token_dict = {item[statistic_id_key]:str2list(item[statistic_object_keys[0]]) for item in token_data}\n",
    "        what_word_dict = {item[statistic_id_key]:str2list(item[statistic_object_keys[2]]) for item in what_word_data}\n",
    "        \n",
    "        statistic_data = []\n",
    "        ids = list(set(list(object_dict.keys()) + list(token_dict.keys()) + list(what_word_dict.keys())))\n",
    "        for id in ids:\n",
    "            objects = list(set(object_dict.get(id,[])))\n",
    "            tokens = token_dict.get(id,[])\n",
    "            what_words = what_word_dict.get(id,[])\n",
    "            co_occurrences = []\n",
    "            for i in range(len(objects)):\n",
    "                for j in range(i+1,len(objects)):\n",
    "                    two_words = get_two_words(objects[i],objects[j])\n",
    "                    co_occurrences.append(two_words)\n",
    "            target_dict=dict(id=id,statistics=dict(object=objects,token=tokens,what_word=what_words,co_occurrence=co_occurrences))\n",
    "            statistic_data.append(target_dict)\n",
    "    \n",
    "    def build_distribution_data(self):\n",
    "        token_input_file,object_input_file,co_occurrence_input_file,what_word_input_file = distribution_reverse_index_files\n",
    "        \n",
    "        \n",
    "        token_data = process_jsonl(token_input_file)\n",
    "        object_data = process_jsonl(object_input_file)\n",
    "        co_occurrence_data = process_jsonl(co_occurrence_input_file)\n",
    "        what_word_data = process_jsonl(what_word_input_file)\n",
    "        \n",
    "        token_data.sort(key=lambda x: len(x[\"ids\"]), reverse=True)\n",
    "        object_data.sort(key=lambda x: len(x[\"ids\"]), reverse=True)\n",
    "        co_occurrence_data.sort(key=lambda x: len(x[\"ids\"]), reverse=True)\n",
    "        what_word_data.sort(key=lambda x: len(x[\"ids\"]), reverse=True)\n",
    "        \n",
    "        token_data = [(x[\"object\"],len(x[\"ids\"])) for x in token_data]\n",
    "        object_data = [(x[\"object\"],len(x[\"ids\"])) for x in object_data]\n",
    "        co_occurrence_data = [(x[\"object\"],len(x[\"ids\"])) for x in co_occurrence_data]\n",
    "        what_word_data = [(x[\"object\"],len(x[\"ids\"])) for x in what_word_data]\n",
    "        \n",
    "        token_data = token_data\n",
    "        object_data = object_data\n",
    "        co_occurrence_data = co_occurrence_data\n",
    "        what_word_data = what_word_data\n",
    "\n",
    "    def get_ratio_index(self):\n",
    "        \n",
    "        def get_90_index(data,sum,ratio=0.9):\n",
    "            sum_90 = sum*ratio\n",
    "            sum_temp = 0\n",
    "            for i in range(len(data)):\n",
    "                sum_temp += data[i][1]\n",
    "                if sum_temp >= sum_90:\n",
    "                    return i\n",
    "                \n",
    "        token_data,object_data,co_occurrence_data,what_word_data = token_data,object_data,co_occurrence_data,what_word_data\n",
    "        token_sum = sum([x[1] for x in token_data])\n",
    "        object_sum = sum([x[1] for x in object_data])\n",
    "        co_occurrence_sum = sum([x[1] for x in co_occurrence_data])\n",
    "        what_word_sum = sum([x[1] for x in what_word_data])\n",
    "        \n",
    "        token_threshold,object_threshold,co_occurrence_threshold,what_word_threshold = distribution_thresholds     \n",
    "        token_90_loc = get_90_index(token_data,token_sum,token_threshold)\n",
    "        object_90_loc = get_90_index(object_data,object_sum,object_threshold)\n",
    "        co_occurrence_90_loc = get_90_index(co_occurrence_data,co_occurrence_sum,co_occurrence_threshold)\n",
    "        what_word_90_loc = get_90_index(what_word_data,what_word_sum,what_word_threshold)\n",
    "        print(f\"token_{token_threshold}_loc:{token_90_loc}, object_{object_threshold}_loc:{object_90_loc}, co_occurrence_{co_occurrence_threshold}_loc:{co_occurrence_90_loc}, what_word_what_word_threshold_loc:{what_word_90_loc}\")\n",
    "        print(f\"token_total {len(token_data)} object_total {len(object_data)} co_occurrence_total {len(co_occurrence_data)} what_word_total {len(what_word_data)}\")\n",
    "        \n",
    "        \n",
    "        token_dict = {x[0]:idx for idx,x in enumerate(token_data)}\n",
    "        object_dict = {x[0]:idx for idx,x in enumerate(object_data)}\n",
    "        co_occurrence_dict = {x[0]:idx for idx,x in enumerate(co_occurrence_data)}\n",
    "        what_word_dict = {x[0]:idx for idx,x in enumerate(what_word_data)}\n",
    "        loc_of_90=[token_90_loc,object_90_loc,co_occurrence_90_loc,what_word_90_loc]\n",
    "    \n",
    "    \n",
    "    def get_head_tail_data_ids(self):\n",
    "        def lookup_to_dict(obj,dictionary):\n",
    "            return dictionary.get(obj,-1)\n",
    "\n",
    "        def get_scores(data,dictionary,threshold):\n",
    "            scores = []\n",
    "            metadata = []\n",
    "            for item in data:\n",
    "                score = lookup_to_dict(item,dictionary)\n",
    "                if score >= 0:\n",
    "                    scores.append(score)\n",
    "                    metadata.append({\"object\":item,\"score\":score})\n",
    "            avg_score = sum(scores)/len(scores) if len(scores) > 0 else 0\n",
    "            max_score = max(scores) if len(scores) > 0 else 0\n",
    "            avg_pass = avg_score >= threshold\n",
    "            least_pass = max_score >= threshold\n",
    "            return scores,metadata,avg_pass,least_pass\n",
    "        \n",
    "        statistic_data = statistic_data\n",
    "        token_90_loc,object_90_loc,co_occurrence_90_loc,what_word_90_loc = loc_of_90\n",
    "        \n",
    "        for item in statistic_data:\n",
    "            tokens = item[\"statistics\"][\"token\"]\n",
    "            objects = item[\"statistics\"][\"object\"]\n",
    "            co_occurrences = item[\"statistics\"][\"co_occurrence\"]\n",
    "            what_words = item[\"statistics\"][\"what_word\"]\n",
    "            \n",
    "            token_scores,token_metadata,token_avg_pass,token_least_pass = get_scores(tokens,token_dict,token_90_loc)\n",
    "            object_scores,object_metadata,object_avg_pass,object_least_pass = get_scores(objects,object_dict,object_90_loc)\n",
    "            co_occurrence_scores,co_occurrence_metadata,co_occurrence_avg_pass,co_occurrence_least_pass = get_scores(co_occurrences,co_occurrence_dict,co_occurrence_90_loc)\n",
    "            what_word_scores,what_word_metadata,what_word_avg_pass,what_word_least_pass = get_scores(what_words,what_word_dict,what_word_90_loc)\n",
    "            distribution_item = dict(token=dict(scores=token_scores,metadata=token_metadata,avg_pass=token_avg_pass,least_pass=token_least_pass),\n",
    "                                    object=dict(scores=object_scores,metadata=object_metadata,avg_pass=object_avg_pass,least_pass=object_least_pass),\n",
    "                                    co_occurrence=dict(scores=co_occurrence_scores,metadata=co_occurrence_metadata,avg_pass=co_occurrence_avg_pass,least_pass=co_occurrence_least_pass),\n",
    "                                    what_word=dict(scores=what_word_scores,metadata=what_word_metadata,avg_pass=what_word_avg_pass,least_pass=what_word_least_pass))\n",
    "            item[\"distribution\"] = distribution_item\n",
    "        \n",
    "        token_pass_cnt = 0\n",
    "        object_pass_cnt = 0\n",
    "        co_occurrence_pass_cnt = 0\n",
    "        what_word_pass_cnt = 0\n",
    "        all_pass_cnt = 0\n",
    "        pass_key = head_tail_pass_key\n",
    "        pass_ids = []\n",
    "        not_pass_ids = []\n",
    "        for item in statistic_data:\n",
    "            token_pass,object_pass,co_occurrence_pass,what_word_pass = 0,0,0,0\n",
    "            if item[\"distribution\"][\"token\"][pass_key]:\n",
    "                token_pass = 1\n",
    "                token_pass_cnt += 1\n",
    "            if item[\"distribution\"][\"object\"][pass_key]:\n",
    "                object_pass = 1\n",
    "                object_pass_cnt += 1\n",
    "            if item[\"distribution\"][\"co_occurrence\"][pass_key]:\n",
    "                co_occurrence_pass_cnt += 1\n",
    "                co_occurrence_pass = 1\n",
    "            if item[\"distribution\"][\"what_word\"][pass_key]:\n",
    "                what_word_pass_cnt += 1\n",
    "                what_word_pass = 1\n",
    "            if token_pass + object_pass + co_occurrence_pass + what_word_pass >= 1:\n",
    "                all_pass_cnt += 1\n",
    "                pass_ids.append(item[statistic_id_key])\n",
    "            else:\n",
    "                not_pass_ids.append(item[statistic_id_key])\n",
    "        pass_ids = pass_ids\n",
    "        not_pass_ids = not_pass_ids\n",
    "        tail_dict = {i:1 for i in pass_ids}\n",
    "        head_dict = {i:1 for i in not_pass_ids}\n",
    "        all_pass_cnt = all_pass_cnt\n",
    "        print(f\"token_pass_cnt:{token_pass_cnt}, object_pass_cnt:{object_pass_cnt}, \\\n",
    "      co_occurrence_pass_cnt:{co_occurrence_pass_cnt}, what_word_pass_cnt:{what_word_pass_cnt}, all_pass_cnt:{all_pass_cnt}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lt",
   "language": "python",
   "name": "lt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
